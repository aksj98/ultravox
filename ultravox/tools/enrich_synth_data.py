import dataclasses
import os
from typing import Optional

import datasets
import openai
import simple_parsing

DEFAULT_TEMPLATE = """Passage: {passage}

Question: {question}

Answer: {answer}

Provide a short explanation to the question given the passage that entails the answer."""

CLIENT = openai.Client()


# This script is used to embellish a dataset with synthetic data generated by a language model.
# Ex: just enrich_ds -d fixie-ai/boolq-audio -s train -b with_explanation -c explanation
@dataclasses.dataclass
class EnrichWithSyntheticDataArgs:
    dataset_name: str = simple_parsing.field(alias="-d")
    dataset_subset: str = simple_parsing.field(default="default", alias="-S")
    dataset_split: Optional[str] = simple_parsing.field(default=None, alias="-s")
    new_column_name: str = simple_parsing.field(default="explanation", alias="-c")
    language_model: str = simple_parsing.field(default="gpt-4o", alias="-m")
    template: str = DEFAULT_TEMPLATE
    num_samples: Optional[int] = simple_parsing.field(default=None, alias="-n")
    num_workers: int = simple_parsing.field(default=16, alias="-w")
    upload_name: Optional[str] = simple_parsing.field(default=None, alias="-u")
    upload_branch: Optional[str] = simple_parsing.field(default="main", alias="-b")
    token: Optional[str] = simple_parsing.field(default=None, alias="-t")

    def __post_init__(self):
        if not self.upload_name:
            self.upload_name = self.dataset_name


def _enrich_sample(sample, template: str, new_col_name: str, language_model: str):
    input_text = template.format(**sample)
    response = CLIENT.chat.completions.create(
        model=language_model,
        messages=[{"role": "user", "content": input_text}],
        max_tokens=128,
        temperature=0,
    )
    sample[new_col_name] = response.choices[0].message.content
    return sample


def main(args: EnrichWithSyntheticDataArgs):
    ds_name = args.dataset_name
    new_col_name = args.new_column_name

    print(
        f'Loading dataset "{ds_name}", new column name: "{new_col_name}", template:\n{args.template}'
    )
    data_dict = datasets.load_dataset(
        ds_name, args.dataset_subset, split=args.dataset_split
    )
    if args.dataset_split:
        data_dict = {args.dataset_split: data_dict}
    for split, ds_split in data_dict.items():
        print(f'Processing split "{split}"...')
        if args.num_samples:
            ds_split = ds_split.select(range(args.num_samples))

        new_split = ds_split.map(
            _enrich_sample,
            num_proc=args.num_workers,
            fn_kwargs={
                "language_model": args.language_model,
                "new_col_name": new_col_name,
                "template": args.template,
            },
        )

        token = args.token or os.environ.get("HF_TOKEN")

        try:
            new_split.push_to_hub(
                args.upload_name,
                config_name=args.dataset_subset,
                split=split,
                token=token,
                revision=args.upload_branch,
            )
        except Exception as e:
            print(f"Failed to push to hub: {e}")

            # If the push fails, save the data locally.
            # I haven't figured out how to handle new columns in the push_to_hub method automatically.
            output_name = f"{split}-00000-of-00001.parquet"
            new_split.to_parquet(output_name)
            print(f"Saved to {output_name}")


if __name__ == "__main__":
    main(simple_parsing.parse(EnrichWithSyntheticDataArgs))
